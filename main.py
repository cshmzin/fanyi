import streamlit as st
import pandas as pd
from apis import baidu,youdao,google
import csv
from st_aggrid import AgGrid
import matplotlib.pyplot as plt
from data_analys import fanyi_language_count, centext_ltp, pos_radar, ner_radar
from annotated_text import annotated_text
import json
from init_nlp_json import pos,ner


# ä½¿ç”¨streamlitæ­å»ºç§‘å­¦æ•°æ®åˆ†æwebç•Œé¢
st.set_page_config(layout="centered", page_icon="ğŸ“", page_title="ç¿»è¯‘å¯¹æ¯”å¹³å°")
st.title("ğŸ“ åŸºäºPythonçš„ç¿»è¯‘å¯¹æ¯”å¹³å°")

def sidebar():
    # ä¾§æ 
    st.sidebar.title("ğŸ’¬ ç›¸å…³ä¿¡æ¯\n") #ç•Œé¢æ ‡é¢˜ï¼ˆä¾§æ ï¼‰
    st.sidebar.markdown('#### å·²å®ç°ç¿»è¯‘å™¨ï¼š') # ç•Œé¢æ–‡æœ¬å†…å®¹ï¼ˆä¾§æ ï¼‰
    st.sidebar.markdown('* ç™¾åº¦') # ç•Œé¢æ–‡æœ¬å†…å®¹ï¼ˆä¾§æ ï¼‰
    st.sidebar.markdown('* è°·æ­Œ') # ç•Œé¢æ–‡æœ¬å†…å®¹ï¼ˆä¾§æ ï¼‰
    st.sidebar.markdown('* æœ‰é“') # ç•Œé¢æ–‡æœ¬å†…å®¹ï¼ˆä¾§æ ï¼‰
    st.sidebar.markdown('#### å·²å®ç°ç¿»è¯‘çš„è¯­è¨€ï¼š') # ç•Œé¢æ–‡æœ¬å†…å®¹ï¼ˆä¾§æ ï¼‰
    st.sidebar.markdown('* è‹±è¯­') # ç•Œé¢æ–‡æœ¬å†…å®¹ï¼ˆä¾§æ ï¼‰
    st.sidebar.markdown('* æ±‰è¯­') # ç•Œé¢æ–‡æœ¬å†…å®¹ï¼ˆä¾§æ ï¼‰
    st.sidebar.markdown('* æ—¥è¯­') # ç•Œé¢æ–‡æœ¬å†…å®¹ï¼ˆä¾§æ ï¼‰
    st.sidebar.markdown('* éŸ©è¯­') # ç•Œé¢æ–‡æœ¬å†…å®¹ï¼ˆä¾§æ ï¼‰
    st.sidebar.markdown('#### å·²å®ç°è‡ªç„¶è¯­è¨€å¤„ç†çš„è¯­è¨€ï¼š') # ç•Œé¢æ–‡æœ¬å†…å®¹ï¼ˆä¾§æ ï¼‰
    st.sidebar.markdown('* æ±‰è¯­') # ç•Œé¢æ–‡æœ¬å†…å®¹ï¼ˆä¾§æ ï¼‰

def analys():
    # ç¿»è¯‘åŠŸèƒ½
    # æ„å»ºç¿»è¯‘é€‰æ‹©å™¨å­—å…¸
    dicts = {'ç™¾åº¦': baidu, 'æœ‰é“': youdao, 'è°·æ­Œ': google}
    # æ„å»ºè¯­ç™¾åº¦ç¿»è¯‘å™¨è¯­è¨€å­—å…¸
    baidu_language = {'è‹±è¯­':'en','æ±‰è¯­':'zh','æ—¥è¯­':'jp','éŸ©è¯­':'kor'}
    # æ„å»ºè¯­æœ‰é“ç¿»è¯‘å™¨è¯­è¨€å­—å…¸
    youdao_language = {'è‹±è¯­': 'en', 'æ±‰è¯­': 'zh-CHS','æ—¥è¯­':'ja','éŸ©è¯­':'ko'}
    # æ„å»ºè¯­googleç¿»è¯‘å™¨è¯­è¨€å­—å…¸
    google_language = {'è‹±è¯­': 'en', 'æ±‰è¯­': 'zh-cn','æ—¥è¯­':'ja','éŸ©è¯­':'ko'}

    # ç”Ÿæˆå¯é€‰è¯­è¨€ç±»å‹
    languages = pd.DataFrame([key for key in baidu_language])

    # æ„å»ºè¯­è¨€é€‰æ‹©å™¨
    st.markdown('#### é€‰æ‹©ç›®æ ‡è¯­è¨€ï¼š') # è¯­è¨€å™¨æ ‡é¢˜ï¼ˆä¸»æ ï¼‰
    option = st.selectbox('',languages)

    st.markdown('#### è¾“å…¥æ–‡æœ¬ï¼š') # è¾“å…¥æ–‡æœ¬æ¡†æ ‡é¢˜ï¼ˆä¸»æ ï¼‰
    # æ„å»ºè¾“å…¥æ–‡æœ¬æ¡†
    context = st.text_area('','',key='content')

    if context: # åˆ¤æ–­æ–‡æœ¬æ¡†ä¸ä¸ºç©ºæ—¶ï¼Œè¿›è¡Œè‡ªåŠ¨åŒ–åˆ†æ
        with st.spinner('wait for it...'):
            pos_dicts, ner_dicts = nlp(context)
            col1, col2, col3 = st.columns(3)
            with col1:
                st.markdown('##### ç™¾åº¦ç¿»è¯‘ç»“æœï¼š')
                # è¿”å›ç™¾åº¦ç¿»è¯‘å™¨åˆ†æç»“æœ
                baidu_result = st.text_area('',dicts['ç™¾åº¦'](context,baidu_language[option]),key='baidu')
            with col2:
                st.markdown('##### æœ‰é“ç¿»è¯‘ç»“æœï¼š')
                # è¿”å›æœ‰é“ç¿»è¯‘å™¨åˆ†æç»“æœ
                youdao_result = st.text_area('',dicts['æœ‰é“'](context,youdao_language[option]) ,key='youdao')
            with col3:
                st.markdown('##### è°·æ­Œç¿»è¯‘ç»“æœï¼š')
                # è¿”å›googleç¿»è¯‘å™¨åˆ†æç»“æœ
                google_result = st.text_area('',dicts['è°·æ­Œ'](context,google_language[option]),key='google')

            st.markdown('##### é€‰æ‹©åˆé€‚çš„ç¿»è¯‘å¼•æ“ï¼š')
            # æ„å»ºç¿»è¯‘ç»“æœå­—å…¸ï¼Œä¾›ç”¨æˆ·é€‰æ‹©
            results = {'ç™¾åº¦':baidu_result,'æœ‰é“':youdao_result,'è°·æ­Œ':google_result}
            fanyi_apis = pd.DataFrame([key for key in results])

            # ç»“æœé€‰æ‹©å™¨
            option2 = st.selectbox('',fanyi_apis)
            btn = st.button('ç¡®å®š')

        if btn: # å½“ç”¨æˆ·é€‰æ‹©ç»“æœåï¼Œè®°å½•ç”¨æˆ·é€‰æ‹©
            with open('data.csv','a',newline='',encoding='utf-8') as f:
                # ä½¿ç”¨csvæ–‡ä»¶å¯¹ç”¨æˆ·é€‰æ‹©ç»“æœè¿›è¡Œè®°å½•
                writer = csv.writer(f)
                # é€šè¿‡è¿½åŠ æ–¹å¼ï¼Œè¿½åŠ ç”¨æˆ·é€‰æ‹©ç»“æœ
                writer.writerow([option2,option,context,results[option2]])

            print(pos_dicts,ner_dicts)
            fanyi_lists = ['ç™¾åº¦', 'æœ‰é“', 'è°·æ­Œ']
            # é€šè¿‡è¿½åŠ æ–¹å¼ï¼Œè¿½åŠ è¯æ€§æ•°é‡
            with open('nlp_analys/nlp_pos.json','r',encoding='utf-8') as f:
                results = json.load(f)
            index = fanyi_lists.index(option2)
            for key in results[index]:
                results[index][key] += pos_dicts[key]
            with open('nlp_analys/nlp_pos.json', 'w', encoding='utf-8') as f:
                json.dump(results,f)

            # é€šè¿‡è¿½åŠ æ–¹å¼ï¼Œè¿½åŠ å®ä½“æ•°é‡
            with open('nlp_analys/nlp_ner.json','r',encoding='utf-8') as f:
                results = json.load(f)
            index = fanyi_lists.index(option2)
            for key in results[index]:
                results[index][key] += ner_dicts[key]
            with open('nlp_analys/nlp_ner.json', 'w', encoding='utf-8') as f:
                json.dump(results, f)

            st.success('æˆåŠŸæ·»åŠ ')

def show():
    # åˆ†æåŠŸèƒ½
    df = pd.read_csv('data.csv',encoding = 'utf-8')
    ### è¯»å–è®°å½•çš„ç”¨æˆ·é€‰æ‹©ä¿¡æ¯å†…å®¹å¹¶æ˜¾ç¤º
    with st.expander("æ˜¾ç¤ºç”¨æˆ·é€‰æ‹©ä¿¡æ¯"):
        AgGrid(df)

    ### å„ç¿»è¯‘å¼•æ“ç¿»è¯‘å„è¯­ç§æ•°é‡æŸ±çŠ¶å›¾
    with st.expander("æ˜¾ç¤ºå„ç¿»è¯‘å¼•æ“ç¿»è¯‘å„è¯­ç§æ•°é‡"):
        plt = fanyi_language_count(df)
        st.pyplot(plt)

    ### å„ç¿»è¯‘å¼•æ“è¯æ€§å’Œå®ä½“ç±»å‹èƒ½åŠ›å›¾
    with st.expander("æ˜¾ç¤ºå„ç¿»è¯‘å¼•æ“èƒ½åŠ›å›¾"):
        plt = pos_radar()
        st.pyplot(plt)
        plt = ner_radar()
        st.pyplot(plt)



def nlp(sents):
    # è‡ªç„¶è¯­è¨€å¤„ç†åŠŸèƒ½
    # ä¾‹å­ï¼šå°æ˜å»æ·±åœ³å‚åŠ äº†ä¸€åœºè…¾è®¯ä¼šè®®
    # contexts.sent_split() åˆ†å¥
    # ['å°æ˜å»æ·±åœ³å‚åŠ äº†ä¸€åœºè…¾è®¯ä¼šè®®']
    # contexts.seg()[0]  åˆ†è¯
    # [['å°æ˜', 'å»', 'æ·±åœ³', 'å‚åŠ ', 'äº†', 'ä¸€', 'åœº', 'è…¾è®¯ä¼šè®®']]
    # contexts.pos() è¯æ€§æ ‡æ³¨
    # [['nh', 'v', 'ns', 'v', 'u', 'm', 'q', 'n']]
    # contexts.ner() å‘½åå®ä½“è¯†åˆ«
    # [[('Nh', 0, 0), ('Ns', 2, 2)]]
    contexts = centext_ltp([sents])
    pos_tags = {'a': 'å½¢å®¹è¯', 'n': 'åè¯', 'v': 'åŠ¨è¯', 'm': 'é‡è¯', 'd': 'å‰¯è¯', 'r': 'ä»£è¯'}
    ner_tags = {'Nh': 'äººå', 'Ni': 'æœºæ„å', 'Ns': 'åœ°å'}
    pos_lists = []
    ner_lists = []
    ner_dicts = {"äººå": 0, "æœºæ„å": 0, "åœ°å": 0}
    pos_dicts = {"å½¢å®¹è¯": 0, "åè¯": 0, "åŠ¨è¯": 0, "é‡è¯": 0, 'ä»£è¯': 0, 'å‰¯è¯': 0}
    # å–åˆ†è¯ã€è¯æ€§æ ‡æ³¨ã€å‘½åå®ä½“è¯†åˆ«ç»“æœè¿›è¡Œæ‹¼æ¥
    for segs, poss, ners in zip(contexts.seg()[0], contexts.pos(), contexts.ner()):
        # ç»„åˆè¯è¯­ä¸è¯æ€§
        for seg, pos in zip(segs, poss):
            pos_lists.append((seg, pos_tags[pos]) if pos in pos_tags else seg)
            if pos in pos_tags: pos_dicts[pos_tags[pos]] += 1
        # æ ‡æ³¨å®ä½“
        ner_segs = ['O'] * len(segs)
        for ner in ners:
            ner_segs[ner[1]] = ner[0]
            ner_dicts[ner_tags[ner[0]]] += 1
        # ç»„åˆè¯è¯­ä¸å®ä½“
        for seg, ner_seg in zip(segs, ner_segs):
            ner_lists.append((seg, ner_tags[ner_seg]) if ner_seg != 'O' else seg)

    with st.expander("æ˜¾ç¤ºè‡ªç„¶è¯­è¨€åˆ†æç»“æœ"):
        st.markdown('##### è¯æ€§æ ‡æ³¨ç»“æœï¼š')
        annotated_text(*pos_lists)
        st.markdown('##### å®ä½“è¯†åˆ«ç»“æœï¼š')
        annotated_text(*ner_lists)

    return pos_dicts,ner_dicts


def main():
    sidebar()
    analys()
    show()

if __name__ == '__main__':
    # df = pd.read_csv('data.csv', encoding='utf-8')
    # ç¬¬ä¸€æ¬¡å¯åŠ¨æ—¶è¿›è¡Œåˆå§‹åŒ–
    # pos(df)
    # ner(df)
    # å¯åŠ¨
    main()


